#### 参考GPT3数据
- token数量(Tokens): 300B
- 模型参数量(Params): 175B
- 训练数据容量(Caps): 50TB
- 单卡峰值算力(PComp): 312 TFLOPS
- 计算效率(eff): 0.4
- 训练卡数(NumCard):1024(张)
- 训练时长(TrainDay): 34(天)
  
#### 估算公式
$$单卡训练时长(天)=\frac{6\times(模型参数量)\times(token数量)}{(单卡峰值算力)\times(计算效率)\times 3600 \times 24}$$
以模型参数量为10B，训练数据容量为1TB（token数量约6B）来估计，单卡训练一轮(过完一遍1TB训练数据)所需时间为：
$$\frac{6\times(10\times10^9)\times(6\times10^9)}{312\times10^{12}\times0.4\times3600\times24}\approx33.38(天)$$



##### 训练数据容量1TB模型参数10B时，算力(卡数)与训练时间对照表
| 卡数(张)   |  训练时长(天)  |
| :-----:  | :----:  |
| 1 |   33.38     |
| 2 |   16.69  |
| 3 |   11.13  |
| 4 |   8.35  |
| 5 |   6.68  |
| 6 |   5.56  |
| 7 |   4.77  |
| 8 |   4.17  |

#### 模型存储内存
- 模型大小=(模型参数量)*(参数类型所占字节数)/(1024^3)(GB)
- 模型权重所占内存=4*(模型参数量)(B--fp32)
- 模型权重所占内存=2*(模型参数量)(B--fp16)
- 优化器状态所占内存=12*(模型参数量)(B--fp32)
- 梯度所占内存=4*(模型参数量)(B--fp32)
- 梯度所占内存=2*(模型参数量)(B--fp16)
- 无重计算的激活内存=token 长度 * batch size * hidden layer 的神经元数量 * 层数(10+24/t+5 * a * token 长度/hidden layer 的神经元数 * t) (B--fp16)
- 选择性重计算的激活内存=token 长度 * batch size * hidden layer 的神经元数量 * 层数(10+24/t) (B--fp16)
- 全部重计算的激活内存=2 * token 长度 * batch size * hidden layer 的神经元数量 * 层数 (B--fp16) 
- -- 重计算的引入也会引起计算成本的增加：2 * token数 * 模型参数 ≤ C（前向传播）≤ 4 * token数 * 模型参数
- -- a 是 transformer 模型中注意力头 (attention heads) 的个数 
- -- t 是张量并行度 (如果无张量并行，则为 1) 
##### 训练ChatGLM-6B 
- 训练总内存=模型内存+优化器内存+激活内存+梯度内存 = 12GB + 72GB + 12Gb + 7.8GB = 103GB
  
##### 推理
- 推理总内存 ≈1.2×模型内存 = 1.2 * 12 GB = 14.4GB